# -*- coding: utf-8 -*-
"""hate_prediction_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BALyeZ3E_va-ko2jf04JV4I686S8rhLq
"""

import numpy as np
import pandas as pd
import torch
import itertools
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import tensorflow as tf
import tensorflow_hub as hub

from sklearn.model_selection import train_test_split

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from keras.layers.embeddings import Embedding

from pytorch_pretrained_bert import BertTokenizer
from pytorch_pretrained_bert.modeling import BertModel

!pip install pytorch_pretrained_bert

def get_bert_embed_matrix():
    bert = BertModel.from_pretrained('bert-base-uncased')
    bert_embeddings = list(bert.children())[0]
    bert_word_embeddings = list(bert_embeddings.children())[0]
    mat = bert_word_embeddings.weight.data.numpy()
    return mat

BERT_EMBEDDINGS = get_bert_embed_matrix()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_data = pd.read_csv('data.csv')
train_labels = np.array(train_data['label'])
train_tokens_ids = []
for i in train_data['file_id']:
    train_tokens_ids.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(i)[:512]))

model = Sequential()
model.add(Embedding(BERT_EMBEDDINGS.shape[0], BERT_EMBEDDINGS.shape[1], input_length=512, weights = [BERT_EMBEDDINGS], trainable = False))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

train_tokens_ids = pad_sequences(train_tokens_ids, dtype = 'int', padding = 'post')

train_label = []
train_final_data = []
for i in range(len(train_labels)):
    if train_labels[i] == '1' or  train_labels[i] == 1:
        train_final_data.append(train_tokens_ids[i].T)
        train_label.append(1)
    elif train_labels[i] == '0' or  train_labels[i] == 0:
        train_final_data.append(train_tokens_ids[i].T)
        train_label.append(0)
    else:
        pass
        #print(train_labels[i])
    
train_X, test_X, train_y, test_y = train_test_split(train_final_data, train_label)

model.fit(np.array(train_X), train_y, epochs = 10)

print(np.array(train_X))

